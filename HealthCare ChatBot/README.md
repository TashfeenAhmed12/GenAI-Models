# üß† Fine-Tuning LLM for Mental Health Dialogue Support

A specialized conversational AI fine-tuned on psychological dialogue patterns to provide empathetic, supportive mental wellness guidance.

---

## üìã Project Overview

This project fine-tunes **Microsoft's Phi-3-mini-4k-instruct** model to simulate natural, empathetic conversations between a psychologist and a patient. Unlike general-purpose chatbots, this model specializes in therapeutic conversation patterns ‚Äî focusing on reflective listening, validating emotions, and suggesting constructive thought reframing.

### Key Features
- ‚úÖ **Empathetic Response Generation**: Trained on therapy-style Q&A exchanges
- ‚úÖ **Parameter-Efficient Fine-Tuning**: Uses LoRA (Low-Rank Adaptation) to train only 0.015% of model parameters
- ‚úÖ **Mental Health Focus**: Specialized in emotional validation and supportive guidance
- ‚úÖ **Lightweight Deployment**: Fine-tuned adapter is only ~2MB vs full model at ~7GB

---

## üéØ Objective

**Goal**: Create an AI-driven conversational agent that can:
1. Engage in mental wellness dialogue with empathy and emotional intelligence
2. Validate user feelings without judgment
3. Suggest healthy coping mechanisms and thought patterns
4. Provide psychoeducation and guide users toward constructive self-reflection

---

## üîß Technical Implementation

### **1. Base Model**
- **Model**: `microsoft/Phi-3-mini-4k-instruct` (3.8B parameters)
- **Type**: Causal Language Model (autoregressive text generation)
- **Optimization**: 8-bit quantization for efficient CPU inference

### **2. Dataset**
- **Source**: Curated mental health conversation dataset (`train.csv`)
- **Format**: Context (patient question) ‚Üí Response (therapist reply)
- **Training Split**: 80% train / 20% evaluation
- **Preprocessing**: 
  - Removed null values
  - Stripped whitespace
  - Random sampling before splitting

### **3. Fine-Tuning Technique: LoRA**

#### Why LoRA?
Instead of updating all 3.8 billion parameters, LoRA injects small trainable adapter matrices into specific layers, reducing:
- **Trainable Parameters**: From 3.8B ‚Üí 589K (0.015%)
- **Storage Requirements**: From ~7GB ‚Üí ~2MB
- **Training Time**: Significantly faster
- **GPU Memory**: Much lower

#### LoRA Configuration
```python
LoraConfig(
    r=1,                    # Low-rank dimension
    lora_alpha=2,           # Scaling factor
    target_modules=["qkv_proj", "o_proj"],  # Attention layers
    lora_dropout=0.1,
    task_type="CAUSAL_LM"
)
```

### **4. Quantization for Deployment**
- **Method**: 8-bit quantization using `BitsAndBytesConfig`
- **Benefits**:
  - Reduces model size from ~15GB ‚Üí ~4GB in memory
  - Speeds up CPU inference by ~2-4x
  - Minimal accuracy loss (<1% typically)

### **5. Prompt Engineering**

Custom system prompt ensures consistent empathetic behavior:

```
"You are a calm, empathetic assistant for mental wellbeing. 
Validate feelings, be non-judgmental, suggest one small next step. 
Do not diagnose. If crisis is indicated, advise contacting 
local emergency services."
```
## üìä Results: Model Comparison

### Test Question
> **"What are 2 healthy ways to deal with anxiety?"**

### üî¥ Base Model Response (Before Fine-Tuning, with 8-bit quantization)
```
1. Mindfulness meditation: Practice mindfulness meditation by finding 
a quiet and comfortable space, closing your eyes, and focusing on your 
breath. Pay attention to your breathing, and try to clear your mind of 
any distracting thoughts. This can help you become more aware of your 
thoughts and emotions, and learn to manage them more effectively.

2. Physical exercise: Engaging in regular physical activity can help 
reduce anxiety by releasing endorphins, which are the body's natural 
mood-lifters. Find an activity that you enjoy, such as walking, running, 
or yoga, and make it a regular part of your routine.
```

**Analysis**: 
- Informational and technically correct
- Straightforward, instruction-manual style
- Lacks emotional validation or acknowledgment
- No personal connection or empathy
- Reads like a textbook rather than a conversation

---

### üü¢ Fine-Tuned Model Response (After LoRA Fine-Tuning, no quantization)
```
1. Mindfulness and Meditation: Engaging in mindfulness practices can 
help you stay grounded in the present moment and reduce the impact of 
anxious thoughts. Try incorporating meditation into your daily routine, 
even if it's just for a few minutes. There are many free resources 
available online that guide you through meditation for anxiety relief.

2. Physical Activity: Exercise is a powerful tool for reducing anxiety. 
It helps release endorphins, which are chemicals in the brain that act 
as natural painkillers and mood elevators. Find an activity you enjoy, 
whether it's walking, yoga, or dancing, and try to make it a regular 
part of your routine.
```

**Analysis**: 
- ‚úÖ More **conversational and engaging** tone
- ‚úÖ Provides **practical implementation tips** ("even if it's just for a few minutes", "free resources available online")
- ‚úÖ **Encouraging language** ("powerful tool", "natural painkillers and mood elevators")
- ‚úÖ **Actionable suggestions** with specific examples (walking, yoga, dancing)
- ‚úÖ More **supportive phrasing** while maintaining accuracy
- ‚úÖ Better **accessibility** by mentioning free online resources

---

## üöÄ Key Improvements

| Aspect | Base Model (Quantized) | Fine-Tuned Model (Full Precision) |
|--------|------------------------|-----------------------------------|
| **Tone** | Clinical, textbook-like | Conversational, supportive |
| **Emotional Validation** | ‚ùå None | ‚úÖ Implicit through encouraging language |
| **Practicality** | General advice | Specific implementation tips |
| **Accessibility** | Standard suggestions | Mentions free online resources |
| **Language Style** | Instruction manual | Engaging, motivational |
| **Actionability** | Moderate | High (with concrete examples) |
| **Conversation Feel** | Robotic | Natural, warm |

### Performance Notes
- **Base Model**: Uses 8-bit quantization for faster CPU inference (~2-4x speed improvement)
- **Fine-Tuned Model**: Runs in full float32 precision for maximum quality
- **Trade-off**: Quantization speeds up inference but fine-tuning improves response quality, tone, and engagement
- **Key Difference**: Fine-tuned model shows more supportive, accessible language while maintaining technical accuracy

---

## üß™ Training Process Overview

1. **Data Loading**: Load conversation dataset (Context ‚Üí Response pairs)
2. **Preprocessing**: Clean, tokenize, and format for chat template
3. **Model Setup**: Load Phi-3 base model + apply LoRA configuration
4. **Tokenization**: Convert conversations to model-ready tensors with label masking
5. **Training**: Fine-tune for using Hugging Face Trainer
6. **Saving**: Export lightweight LoRA adapter 
7. **Inference**: Load adapter on base model for deployment

---

## üéì Key Learnings

### Why LoRA?
- Traditional fine-tuning updates billions of parameters ‚Üí expensive
- LoRA updates only few
- Maintains base model performance while specializing behavior

### System Prompt Engineering
- Defines consistent personality and safety guardrails
- Injected into every conversation during training and inference
- Critical for maintaining empathetic, non-diagnostic behavior

