{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Conversational LLM for Psychological Dialogue Support\n",
    "\n",
    "This project fine-tunes a Large Language Model (LLM) to simulate natural, empathetic, and context-aware conversations between a psychologist and a patient. The model learns from a curated dataset of real or synthetic therapy-style question‚Äìanswer exchanges, allowing it to generate emotionally intelligent, coherent, and supportive responses to mental-health-related queries.\n",
    "\n",
    "Unlike general chatbots trained on open-domain text, this model specializes in therapeutic conversation patterns ‚Äî focusing on reflective listening, validating emotions, and suggesting healthy thought reframing.\n",
    "\n",
    "The final output is an AI-driven conversational agent that can engage in mental-wellness dialogue, provide psychoeducation, and guide users toward constructive self-reflection ‚Äî without offering clinical diagnosis or treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 CPU threads\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Set number of threads (adjust based on your CPU cores)\n",
    "num_cores = os.cpu_count()  # Get total cores\n",
    "torch.set_num_threads(num_cores)  # Use all cores\n",
    "torch.set_num_interop_threads(num_cores)\n",
    "\n",
    "# For Intel CPUs, these can help too:\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_cores)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(num_cores)\n",
    "\n",
    "print(f\"Using {num_cores} CPU threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from transformers import pipeline\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import datasets\n",
    "\n",
    "from transformers.utils import logging as hf_logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset and Preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataset: 8\n",
      "Length of evaluation dataset: 2\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "df_fine_tuning = pd.read_csv('train.csv')\n",
    "df_fine_tuning = df_fine_tuning.dropna()\n",
    "\n",
    "df_fine_tuning = df_fine_tuning.dropna().apply(lambda x: x.str.strip())\n",
    "df_fine_tuning = df_fine_tuning.sample(n=10, random_state=42)  # ‚úÖ Decreased from 200 to 10 samples\n",
    "# Shuffle and split the dataset\n",
    "train_df = df_fine_tuning.sample(frac=0.8, random_state=42)\n",
    "eval_df = df_fine_tuning.drop(train_df.index)\n",
    "\n",
    "#length of each splits dataset\n",
    "print(f\"Length of training dataset: {len(train_df)}\")\n",
    "print(f\"Length of evaluation dataset: {len(eval_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>I‚Äôve been on 0.5 mg of Xanax twice a day for the past month. It hasn't been helping me at all, but when I take 1 mg during a big anxiety attack, it calms me down. I was wondering how I can ask my psychologist to up the dose to 1 mg twice a day without her thinking I'm abusing them. I just have very big anxiety attacks. Should I stay on the 0.5mg and deal with the attacks or should I ask to up the dose? I'm afraid she will take me off them and put me on something else.</td>\n",
       "      <td>Do you think you're abusing xanax?It is a highly addictive drug so maybe one reason you feel compelled to take more is bc you already are addicted.Drugs don't do anything helpful in solving life's problems. ¬† Once the effect wears off, the stressful situation is once again waiting for you to address it.Think over your reason for not directly asking your psychologist about upping your dose.Also, do you ever talk about your life problems with this psychologist or only your need for drugs? ¬† ¬†The more gradual path to a better life is to not need drugs in the first place. This consists of your willingness to face the matters that are creating such terrible feelings inside you.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Context  \\\n",
       "506  I‚Äôve been on 0.5 mg of Xanax twice a day for the past month. It hasn't been helping me at all, but when I take 1 mg during a big anxiety attack, it calms me down. I was wondering how I can ask my psychologist to up the dose to 1 mg twice a day without her thinking I'm abusing them. I just have very big anxiety attacks. Should I stay on the 0.5mg and deal with the attacks or should I ask to up the dose? I'm afraid she will take me off them and put me on something else.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Response  \n",
       "506  Do you think you're abusing xanax?It is a highly addictive drug so maybe one reason you feel compelled to take more is bc you already are addicted.Drugs don't do anything helpful in solving life's problems. ¬† Once the effect wears off, the stressful situation is once again waiting for you to address it.Think over your reason for not directly asking your psychologist about upping your dose.Also, do you ever talk about your life problems with this psychologist or only your need for drugs? ¬† ¬†The more gradual path to a better life is to not need drugs in the first place. This consists of your willingness to face the matters that are creating such terrible feelings inside you.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and test model without any finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Detected 8 CPU cores\n",
      "‚úÖ Configured to use all 8 cores with Intel optimizations\n",
      "üì¶ Loading model (this may take a minute)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c419eaba8b3a452a9d45607043b57d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "ü§ñ Generating response (watch Task Manager - CPU should spike!)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\LLM\\GenAI-Models\\HealthCare ChatBot\\chatbot\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Bot: 1. Mindfulness meditation: Practice mindfulness meditation by finding a quiet and comfortable space, closing your eyes, and focusing on your breath. Pay attention to your breathing, and try to clear your mind of any distracting thoughts. This can help you become more aware of your thoughts and emotions, and learn to manage them more effectively.\n",
      "\n",
      "2. Physical exercise: Engaging in regular physical activity can help reduce anxiety by releasing endorphins, which are the body's natural mood-lifters. Find an activity that you enjoy, such as walking, running, or yoga, and make it a regular part of your routine.\n",
      "\n",
      "3. T\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== MUST BE AT THE VERY TOP - BEFORE ANY IMPORTS =====\n",
    "import os\n",
    "\n",
    "num_cores = os.cpu_count()  # Get all available CPU cores\n",
    "print(f\"üîß Detected {num_cores} CPU cores\")\n",
    "\n",
    "# Set environment variables BEFORE importing torch\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_cores)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(num_cores)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(num_cores)\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(num_cores)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(num_cores)\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"0\"\n",
    "os.environ[\"KMP_AFFINITY\"] = \"granularity=fine,compact,1,0\"\n",
    "\n",
    "# NOW import torch and other libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Set PyTorch threads (must be before any model operations)\n",
    "torch.set_num_threads(num_cores)\n",
    "# Remove or comment out this line if it still causes issues:\n",
    "# torch.set_num_interop_threads(num_cores)\n",
    "\n",
    "print(f\"‚úÖ Configured to use all {num_cores} cores with Intel optimizations\")\n",
    "# ========================================================\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Define quantization configuration for 8-bit loading (better for CPU)\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Use 8-bit quantization (better CPU support)\n",
    "    llm_int8_threshold=6.0  # Helps with outlier features\n",
    ")\n",
    "\n",
    "print(\"üì¶ Loading model (this may take a minute)...\")\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",  # Will use CPU automatically\n",
    "    low_cpu_mem_usage=True,  # Reduces CPU memory spikes during loading\n",
    "    torch_dtype=torch.float32  # Use float32 for CPU (better compatibility)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# Ask a simple question\n",
    "question = \"What are 2 healthy ways to deal with anxiety?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a calm, empathetic assistant that offers short, clear mental wellness advice.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(\"ü§ñ Generating response (watch Task Manager - CPU should spike!)...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        num_beams=1,  # Greedy decoding is faster on CPU\n",
    "        use_cache=True  # Cache key/value pairs for faster generation\n",
    "    )\n",
    "\n",
    "reply = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Bot:\", reply.strip())\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Finetune the model and see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Dataset\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[[\"Context\", \"Response\"]].reset_index(drop=True))\n",
    "eval_ds  = Dataset.from_pandas(eval_df[[\"Context\", \"Response\"]].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Model and its tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"   # new base\n",
    "MAX_LEN = 384\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e071eaefbd48efab43296f109aad3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589,824 || all params: 3,821,669,376 || trainable%: 0.0154\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.truncation_side = \"right\"\n",
    "tokenizer.model_max_length = MAX_LEN\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False  # needed for training\n",
    "\n",
    "# Correct LoRA targets for Phi-3\n",
    "target_modules = [\"qkv_proj\", \"o_proj\"] # \"gate_up_proj\", \"down_proj\" for feedforward layers require more training resources\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=1, # (rank): the adapter‚Äôs low-rank size. Higher r ‚áí more capacity, more parameters\n",
    "    lora_alpha=2, # scaling factor\n",
    "    target_modules=target_modules, # modules to apply LoRA to\n",
    "    lora_dropout=0.1, # dropout for regularization\n",
    "    bias=\"none\", # no bias modification\n",
    "    task_type=\"CAUSAL_LM\" # task type for causal language modeling\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- System prompt for consistent tone ---\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a calm, empathetic assistant for mental wellbeing. \"\n",
    "    \"Validate feelings, be non-judgmental, suggest one small next step. \"\n",
    "    \"Do not diagnose. If crisis is indicated, advise contacting local emergency services.\"\n",
    ")\n",
    "\n",
    "\n",
    "def encode_row(example):\n",
    "    \"\"\"\n",
    "    Convert one (Context ‚Üí Response) pair from your dataset\n",
    "    into tokenized model-ready tensors for fine-tuning a chat model.\n",
    "\n",
    "    Input:\n",
    "        example: a dictionary-like object with keys:\n",
    "                 \"Context\"  - what the user said (the question)\n",
    "                 \"Response\" - what the assistant (therapist) replied\n",
    "\n",
    "    Output:\n",
    "        A dictionary containing:\n",
    "          - input_ids: token IDs of the full conversation\n",
    "          - attention_mask: mask for real vs padded tokens\n",
    "          - labels: same as input_ids but with prompt tokens masked as -100\n",
    "                    (so loss is only computed on assistant‚Äôs reply)\n",
    "    \"\"\"\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1Ô∏è‚É£ Build the \"full conversation\" message list (system + user + assistant)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SYSTEM_PROMPT provides consistent tone/behavior.\n",
    "    # The user and assistant parts come from your dataset row.\n",
    "    messages_full = [\n",
    "        {\"role\": \"system\",    \"content\": SYSTEM_PROMPT},      # defines model personality\n",
    "        {\"role\": \"user\",      \"content\": example[\"Context\"]}, # user question/input\n",
    "        {\"role\": \"assistant\", \"content\": example[\"Response\"]} # correct reply to learn\n",
    "    ]\n",
    "\n",
    "    # Convert that structured list into plain text formatted for Phi-3.\n",
    "    # Example output:\n",
    "    #   <|system|>You are calm...\n",
    "    #   <|user|>I feel anxious\n",
    "    #   <|assistant|>That‚Äôs understandable...\n",
    "    text_full = tokenizer.apply_chat_template(\n",
    "        messages_full,\n",
    "        tokenize=False,             # return as string, not token IDs yet\n",
    "        add_generation_prompt=False # don't append an empty assistant header\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2Ô∏è‚É£ Build the \"prompt-only\" version (system + user only, no assistant text)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # This helps us identify how long the prompt is in tokens,\n",
    "    # so we can later mask that region in the labels.\n",
    "    messages_prompt = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": example[\"Context\"]}\n",
    "    ]\n",
    "\n",
    "    # Setting add_generation_prompt=True tells the tokenizer to append\n",
    "    # the \"assistant\" header ‚Äî basically where generation will begin.\n",
    "    prompt_only = tokenizer.apply_chat_template(\n",
    "        messages_prompt,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3Ô∏è‚É£ Tokenize both versions (full and prompt)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Convert the text into token IDs that the model understands.\n",
    "    # We truncate to MAX_LEN (to fit model context) and pad shorter ones.\n",
    "    # return_tensors=\"pt\" gives PyTorch tensors directly.\n",
    "    enc_full = tokenizer(\n",
    "        text_full,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    enc_prompt = tokenizer(\n",
    "        prompt_only,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4Ô∏è‚É£ Extract token IDs and attention masks from encodings\n",
    "    # -------------------------------------------------------------------------\n",
    "    input_ids = enc_full[\"input_ids\"][0]          # the actual tokens (numbers)\n",
    "    attn_mask = enc_full[\"attention_mask\"][0]     # 1 = real token, 0 = padding\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5Ô∏è‚É£ Create labels for training (same as input_ids initially)\n",
    "    # -------------------------------------------------------------------------\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6Ô∏è‚É£ Mask out the prompt tokens (system + user)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # We compute how many tokens belong to the prompt part.\n",
    "    # We use the attention mask of the \"prompt-only\" encoding to count them.\n",
    "    prompt_len = int((enc_prompt[\"attention_mask\"][0]).sum().item())\n",
    "\n",
    "    # For all tokens that belong to the system+user part,\n",
    "    # we set label = -100 so the loss is ignored on them.\n",
    "    # Only the assistant's part will be used for loss calculation.\n",
    "    labels[:prompt_len] = -100\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 7Ô∏è‚É£ Return the dictionary that the Trainer expects\n",
    "    # -------------------------------------------------------------------------\n",
    "    return {\n",
    "        \"input_ids\": input_ids,           # tokenized full conversation\n",
    "        \"attention_mask\": attn_mask,      # mask for real tokens vs padding\n",
    "        \"labels\": labels                  # same as input_ids but masked\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df38765a41ea48c2b5f28f39b522a4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c98a419924b4b5985e809ff498b497a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenized = train_ds.map(encode_row)\n",
    "eval_tokenized  = eval_ds.map(encode_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set format for PyTorch\n",
    "cols = [\"input_ids\",\"attention_mask\",\"labels\"]\n",
    "train_tokenized.set_format(type=\"torch\", columns=cols)\n",
    "eval_tokenized.set_format(type=\"torch\", columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Current device: CPU only\n"
     ]
    }
   ],
   "source": [
    "# CHEKC IF GPU IS AVAILABLE\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"CPU only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the Training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: Response, Context. If Response, Context are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 8\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12\n",
      "  Number of trainable parameters = 589,824\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 49:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.842300</td>\n",
       "      <td>4.415180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.528400</td>\n",
       "      <td>4.318367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.382900</td>\n",
       "      <td>4.270855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: Response, Context. If Response, Context are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 2\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./results\\checkpoint-4\n",
      "Saving model checkpoint to ./results\\checkpoint-4\n",
      "loading configuration file config.json from cache at C:\\Users\\Tashfeen Ahmed\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-4k-instruct\\snapshots\\0a67737cc96d2554230f90338b163bc6380a2a85\\config.json\n",
      "Model config Phi3Config {\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"partial_rotary_factor\": 1.0,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\Tashfeen Ahmed\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-4k-instruct\\snapshots\\0a67737cc96d2554230f90338b163bc6380a2a85\\config.json\n",
      "Model config Phi3Config {\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"partial_rotary_factor\": 1.0,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: Response, Context. If Response, Context are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./results\\checkpoint-8\n",
      "loading configuration file config.json from cache at C:\\Users\\Tashfeen Ahmed\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-4k-instruct\\snapshots\\0a67737cc96d2554230f90338b163bc6380a2a85\\config.json\n",
      "Model config Phi3Config {\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"partial_rotary_factor\": 1.0,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "The following columns in the Evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: Response, Context. If Response, Context are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./results\\checkpoint-12\n",
      "loading configuration file config.json from cache at C:\\Users\\Tashfeen Ahmed\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-4k-instruct\\snapshots\\0a67737cc96d2554230f90338b163bc6380a2a85\\config.json\n",
      "Model config Phi3Config {\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"partial_rotary_factor\": 1.0,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "Deleting older checkpoint [results\\checkpoint-4] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=6.7316296100616455, metrics={'train_runtime': 3220.6364, 'train_samples_per_second': 0.007, 'train_steps_per_second': 0.004, 'total_flos': 205876340195328.0, 'train_loss': 6.7316296100616455, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Training args \n",
    "hf_logging.set_verbosity_info()\n",
    "datasets.logging.set_verbosity_info()\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  # ‚úÖ Reduced from 16 to 2 for more visible steps\n",
    "    per_device_eval_batch_size=2,   # ‚úÖ Reduced for consistency\n",
    "    gradient_accumulation_steps=1,  \n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,                # ‚úÖ Log every step\n",
    "    logging_first_step=True, \n",
    "    logging_strategy=\"steps\", \n",
    "    dataloader_pin_memory=False,\n",
    "    disable_tqdm=False,             # ‚úÖ Keep progress bars enabled\n",
    "    report_to=\"none\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=eval_tokenized,\n",
    ")\n",
    "\n",
    "# # --- Print step counts before training ---\n",
    "# num_samples = len(train_tokenized)\n",
    "# batch_size = args.per_device_train_batch_size\n",
    "# grad_accum = args.gradient_accumulation_steps\n",
    "# epochs = args.num_train_epochs\n",
    "\n",
    "# steps_per_epoch = (num_samples + (batch_size * grad_accum) - 1) // (batch_size * grad_accum)\n",
    "# total_steps = steps_per_epoch * epochs\n",
    "\n",
    "# print(f\"üìä Dataset size: {num_samples} samples\")\n",
    "# print(f\"üß© Effective batch size: {batch_size * grad_accum}\")\n",
    "# print(f\"üîÅ Steps per epoch: {steps_per_epoch}\")\n",
    "# print(f\"‚è±Ô∏è Total training steps: {total_steps}\\n\")\n",
    "\n",
    "# --- Train ---\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Tashfeen Ahmed\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-4k-instruct\\snapshots\\0a67737cc96d2554230f90338b163bc6380a2a85\\config.json\n",
      "Model config Phi3Config {\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"partial_rotary_factor\": 1.0,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "Model config Phi3Config {\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"partial_rotary_factor\": 1.0,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "chat template saved in ./lora_finetuned_model\\chat_template.jinja\n",
      "tokenizer config file saved in ./lora_finetuned_model\\tokenizer_config.json\n",
      "Special tokens file saved in ./lora_finetuned_model\\special_tokens_map.json\n",
      "chat template saved in ./lora_finetuned_model\\chat_template.jinja\n",
      "tokenizer config file saved in ./lora_finetuned_model\\tokenizer_config.json\n",
      "Special tokens file saved in ./lora_finetuned_model\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA adapter saved to ./lora_finetuned_model\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Save LoRA adapter (small) ---\n",
    "os.makedirs(\"./lora_finetuned_model\", exist_ok=True)\n",
    "trainer.model.save_pretrained(\"./lora_finetuned_model\") #saves the finetuned parameters lora adapters\n",
    "tokenizer.save_pretrained(\"./lora_finetuned_model\")\n",
    "print(\"‚úÖ LoRA adapter saved to ./lora_finetuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the fine tuned model and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuring to use all 8 CPU cores...\n",
      "‚úÖ CPU configured with Intel optimizations\n",
      "üì¶ Loading base model (float32, no quantization)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6df15d3ee74b4e8b1b45ea05330ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading LoRA adapter...\n",
      "‚úÖ Fine-tuned model loaded successfully!\n",
      "‚úÖ Fine-tuned model loaded successfully!\n",
      "ü§ñ Generating fine-tuned response...\n",
      "ü§ñ Generating fine-tuned response...\n",
      "\n",
      "============================================================\n",
      "üß† FINE-TUNED MODEL RESPONSE\n",
      "============================================================\n",
      "Question: What are 3 healthy ways to deal with anxiety?\n",
      "\n",
      "Response: Dealing with anxiety can be challenging, but there are healthy ways to manage it. Here are three strategies that might help:\n",
      "\n",
      "\n",
      "1. Mindfulness and Meditation: Engaging in mindfulness practices can help you stay grounded in the present moment and reduce the impact of anxious thoughts. Try incorporating meditation into your daily routine, even if it's just for a few minutes. There are many free resources available online that guide you through meditation for anxiety relief.\n",
      "\n",
      "\n",
      "2. Physical Activity: Exercise is a powerful tool for reducing anxiety. It helps release endorphins, which are chemicals in the brain that act as natural painkillers and mood\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üß† FINE-TUNED MODEL RESPONSE\n",
      "============================================================\n",
      "Question: What are 3 healthy ways to deal with anxiety?\n",
      "\n",
      "Response: Dealing with anxiety can be challenging, but there are healthy ways to manage it. Here are three strategies that might help:\n",
      "\n",
      "\n",
      "1. Mindfulness and Meditation: Engaging in mindfulness practices can help you stay grounded in the present moment and reduce the impact of anxious thoughts. Try incorporating meditation into your daily routine, even if it's just for a few minutes. There are many free resources available online that guide you through meditation for anxiety relief.\n",
      "\n",
      "\n",
      "2. Physical Activity: Exercise is a powerful tool for reducing anxiety. It helps release endorphins, which are chemicals in the brain that act as natural painkillers and mood\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Load the fine-tuned LoRA model and test it ---\n",
    "\n",
    "# ===== CPU Optimization - Use all cores =====\n",
    "import os\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"üîß Configuring to use all {num_cores} CPU cores...\")\n",
    "\n",
    "# Set environment variables for Intel CPU optimization\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_cores)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(num_cores)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(num_cores)\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(num_cores)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(num_cores)\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"0\"\n",
    "os.environ[\"KMP_AFFINITY\"] = \"granularity=fine,compact,1,0\"\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(num_cores)\n",
    "\n",
    "print(f\"‚úÖ CPU configured with Intel optimizations\")\n",
    "# ============================================\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a calm, empathetic assistant for mental wellbeing. \"\n",
    "    \"Validate feelings, be non-judgmental, suggest one small next step. \"\n",
    "    \"Do not diagnose. If crisis is indicated, advise contacting local emergency services.\"\n",
    ")\n",
    "MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"üì¶ Loading base model (float32, no quantization)...\")\n",
    "# Load base model without quantization\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "print(\"üîß Loading LoRA adapter...\")\n",
    "# Load LoRA adapter on top of base model\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, \"./lora_finetuned_model\")\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(\"./lora_finetuned_model\")\n",
    "\n",
    "print(\"‚úÖ Fine-tuned model loaded successfully!\")\n",
    "\n",
    "# Test with the SAME question as the base model\n",
    "test_question = \"What are 2 healthy ways to deal with anxiety?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": test_question}\n",
    "]\n",
    "\n",
    "# Prepare input\n",
    "prompt = finetuned_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = finetuned_tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(\"ü§ñ Generating fine-tuned response...\")\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    outputs = finetuned_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=finetuned_tokenizer.eos_token_id,\n",
    "        eos_token_id=finetuned_tokenizer.eos_token_id,\n",
    "        num_beams=1,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "# Decode and print response\n",
    "response = finetuned_tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üß† FINE-TUNED MODEL RESPONSE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"\\nResponse: {response.strip()}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
